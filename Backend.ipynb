{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkj5fH79iT6NVlUylVGVOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abudubai16/saras-ai-hackathon/blob/main/Backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdWvoh-Uy-xi",
        "outputId": "cc4f0585-e5d2-4cfd-93b2-f9100c3574ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "Og1UKA6A008g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "PATH = '/content'\n",
        "\n",
        "\n",
        "def convert_repo_to_embedding(path:str) -> dict:\n",
        "  img_model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "  image_paths = os.listdir(path)\n",
        "  embeddings = []\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    embedding = img_model.encode([Image.open(os.path.join(PATH, image_path))])\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "  del img_model\n",
        "\n",
        "  return {image_path: embedding for image_path, embedding in zip(image_paths, embeddings)}"
      ],
      "metadata": {
        "id": "LABx99UT01i3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_embeddings_db(embeddings: dict)-> None:\n",
        "  pass"
      ],
      "metadata": {
        "id": "0c9KK7xK3PWV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_highest_similarity_image(text:str, top_n:int)->list:\n",
        "\n",
        "  min = 0\n",
        "  text_embed = get_text_embedding(text)\n",
        "  pass"
      ],
      "metadata": {
        "id": "mUhke89L4Ts0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = lambda vector: ((vector - vector.mean()) / vector.std())\n",
        "\n",
        "def get_similarity_score(vector1:np.array, vector2: np.array)-> float:\n",
        "  return np.dot(normalize(vector1), normalize(vector2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MdYdAsd43Gu",
        "outputId": "6300a00d-476b-47d4-daa1-76ca87b3109d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_embedding(text):\n",
        "\n",
        "  model_name = 'openai/clip-vit-base-patch32'\n",
        "  tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
        "  model = CLIPTextModel.from_pretrained(model_name)\n",
        "\n",
        "  \"\"\"Generate an embedding for the given text.\"\"\"\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "  # Extract the last hidden state\n",
        "  embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "  del tokenizer\n",
        "  del model\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "SNQKzLRE3t1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}